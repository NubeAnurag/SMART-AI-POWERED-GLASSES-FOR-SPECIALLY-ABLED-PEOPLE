import cv2
import numpy as np
import face_recognition
import os
import pickle
import time
from datetime import datetime
from config import KNOWN_PERSONS, SYSTEM_CONFIG, DISPLAY_CONFIG, EMOTION_DISPLAY

class SimpleHumanDetectionSystem:
    def __init__(self):
        self.known_faces = {}
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        self.cap = None
        self.fps_counter = 0
        self.fps_start_time = time.time()
        
        # Load known faces database
        self.load_known_faces()
        
    def load_known_faces(self):
        """Load known faces from database or create sample data"""
        # Initialize known faces from config
        self.known_faces = {}
        for person_id, person_data in KNOWN_PERSONS.items():
            self.known_faces[person_id] = {
                "name": person_data["name"],
                "age": person_data["age"],
                "gender": person_data["gender"],
                "description": person_data["description"],
                "encoding": None  # Will be set when face is detected
            }
        
        # Create known_faces directory if it doesn't exist
        if not os.path.exists('known_faces'):
            os.makedirs('known_faces')
            
        # Load face encodings if they exist
        if os.path.exists('known_faces/encodings.pkl'):
            with open('known_faces/encodings.pkl', 'rb') as f:
                saved_encodings = pickle.load(f)
                for name, encoding in saved_encodings.items():
                    if name in self.known_faces:
                        self.known_faces[name]["encoding"] = encoding
    
    def detect_human(self, frame):
        """Detect if there's a human face in the frame"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.1, 4)
        return len(faces) > 0, faces
    
    def recognize_person(self, face_encoding):
        """Recognize if the person is known"""
        if face_encoding is None:
            return None, 0
            
        for name, data in self.known_faces.items():
            if data["encoding"] is not None:
                # Compare face encodings
                matches = face_recognition.compare_faces([data["encoding"]], face_encoding, tolerance=SYSTEM_CONFIG["face_detection_confidence"])
                if matches[0]:
                    return name, face_recognition.face_distance([data["encoding"]], face_encoding)[0]
        return None, 1.0
    
    def estimate_gender_age(self, face_image):
        """Simple gender and age estimation based on face features"""
        # This is a simplified estimation - in a real system you'd use trained models
        # For now, we'll use basic heuristics based on face proportions
        
        gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        
        # Detect facial features (only eyes to avoid cascade file issues)
        eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
        eyes = eye_cascade.detectMultiScale(gray, 1.1, 5)
        
        # Simple heuristics (this is just for demonstration)
        # In reality, you'd use trained ML models for accurate results
        
        # Estimate age based on face size and features
        face_area = face_image.shape[0] * face_image.shape[1]
        estimated_age = 25 + (face_area % 20)  # Random-ish but consistent for same face
        
        # Estimate gender (simplified)
        # In reality, this would be based on facial features, jawline, etc.
        gender = "Male" if len(eyes) > 1 else "Female"
        
        return {
            'age': estimated_age,
            'gender': gender,
            'emotion': 'neutral'  # Default emotion
        }
    
    def start_detection(self):
        """Start the human detection system"""
        self.cap = cv2.VideoCapture(SYSTEM_CONFIG["camera_index"])
        
        if not self.cap.isOpened():
            print("Error: Could not open webcam")
            return
            
        print("Human Detection System Started!")
        print("Press 'q' to quit, 's' to save current face, 'h' for help")
        
        while True:
            ret, frame = self.cap.read()
            if not ret:
                print("Error: Could not read frame")
                break
                
            # Flip frame horizontally for mirror effect
            frame = cv2.flip(frame, 1)
            
            # Detect human faces
            is_human, face_locations = self.detect_human(frame)
            
            if is_human:
                # Get face encodings using face_recognition
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                face_encodings = face_recognition.face_encodings(rgb_frame)
                face_locations_recognition = face_recognition.face_locations(rgb_frame)
                
                for i, (face_encoding, face_location) in enumerate(zip(face_encodings, face_locations_recognition)):
                    # Analyze face
                    analysis = self.estimate_gender_age(frame)
                    
                    # Recognize person
                    recognized_name, confidence = self.recognize_person(face_encoding)
                    
                    # Draw rectangle around face
                    top, right, bottom, left = face_location
                    box_color = DISPLAY_CONFIG["box_color"] if recognized_name else DISPLAY_CONFIG["unknown_color"]
                    cv2.rectangle(frame, (left, top), (right, bottom), box_color, DISPLAY_CONFIG["line_thickness"])
                    
                    # Prepare display text
                    if recognized_name:
                        person_data = self.known_faces[recognized_name]
                        display_text = f"I recognize this person!"
                        name_text = f"Name: {person_data['name']}"
                        age_text = f"Age: {person_data['age']}"
                        gender_text = f"Gender: {person_data['gender']}"
                        emotion_text = f"Emotion: {EMOTION_DISPLAY.get(analysis['emotion'], analysis['emotion'])}"
                        confidence_text = f"Confidence: {1-confidence:.2f}"
                    else:
                        display_text = "I don't know this person"
                        name_text = f"Detected Gender: {analysis['gender']}"
                        age_text = f"Estimated Age: {analysis['age']}"
                        emotion_text = f"Emotion: {EMOTION_DISPLAY.get(analysis['emotion'], analysis['emotion'])}"
                        gender_text = ""
                        confidence_text = ""
                    
                    # Display text on frame
                    y_position = top - 10
                    cv2.putText(frame, display_text, (left, y_position), 
                              cv2.FONT_HERSHEY_SIMPLEX, DISPLAY_CONFIG["font_scale"], 
                              DISPLAY_CONFIG["text_color"], DISPLAY_CONFIG["font_thickness"])
                    
                    y_position -= 30
                    if name_text:
                        cv2.putText(frame, name_text, (left, y_position), 
                                  cv2.FONT_HERSHEY_SIMPLEX, DISPLAY_CONFIG["font_scale"], 
                                  DISPLAY_CONFIG["text_color"], DISPLAY_CONFIG["font_thickness"])
                        y_position -= 25
                    
                    if age_text:
                        cv2.putText(frame, age_text, (left, y_position), 
                                  cv2.FONT_HERSHEY_SIMPLEX, DISPLAY_CONFIG["font_scale"], 
                                  DISPLAY_CONFIG["text_color"], DISPLAY_CONFIG["font_thickness"])
                        y_position -= 25
                    
                    if gender_text:
                        cv2.putText(frame, gender_text, (left, y_position), 
                                  cv2.FONT_HERSHEY_SIMPLEX, DISPLAY_CONFIG["font_scale"], 
                                  DISPLAY_CONFIG["text_color"], DISPLAY_CONFIG["font_thickness"])
                        y_position -= 25
                    
                    if emotion_text:
                        cv2.putText(frame, emotion_text, (left, y_position), 
                                  cv2.FONT_HERSHEY_SIMPLEX, DISPLAY_CONFIG["font_scale"], 
                                  DISPLAY_CONFIG["text_color"], DISPLAY_CONFIG["font_thickness"])
                        y_position -= 25
                    
                    if confidence_text:
                        cv2.putText(frame, confidence_text, (left, y_position), 
                                  cv2.FONT_HERSHEY_SIMPLEX, DISPLAY_CONFIG["font_scale"], 
                                  DISPLAY_CONFIG["text_color"], DISPLAY_CONFIG["font_thickness"])
            else:
                # No human detected
                cv2.putText(frame, "No human detected", (10, 30), 
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            
            # Calculate and display FPS
            if SYSTEM_CONFIG["display_fps"]:
                self.fps_counter += 1
                if time.time() - self.fps_start_time >= 1.0:
                    fps = self.fps_counter / (time.time() - self.fps_start_time)
                    self.fps_counter = 0
                    self.fps_start_time = time.time()
                    cv2.putText(frame, f"FPS: {fps:.1f}", (10, frame.shape[0] - 10), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
            
            # Display frame
            cv2.imshow(DISPLAY_CONFIG["window_title"], frame)
            
            # Handle key presses
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                self.save_current_face(frame, face_encodings, face_locations_recognition)
            elif key == ord('h'):
                self.show_help()
        
        self.cleanup()
    
    def save_current_face(self, frame, face_encodings, face_locations):
        """Save current face to known faces database"""
        if not face_encodings:
            print("No face detected to save")
            return
            
        # Save the first detected face
        face_encoding = face_encodings[0]
        face_location = face_locations[0]
        
        # Generate unique name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        new_name = f"person_{timestamp}"
        
        # Add to known faces
        self.known_faces[new_name] = {
            "name": f"Person {timestamp}",
            "age": "Unknown",
            "gender": "Unknown",
            "description": "Saved from webcam",
            "encoding": face_encoding
        }
        
        # Save face image
        top, right, bottom, left = face_location
        face_image = frame[top:bottom, left:right]
        cv2.imwrite(f"saved_faces/{new_name}.jpg", face_image)
        
        # Save encoding
        self.save_encodings()
        
        print(f"‚úÖ Face saved as {new_name}")
        print(f"üìÅ Image saved to: saved_faces/{new_name}.jpg")
    
    def show_help(self):
        """Show help information"""
        print("\n" + "="*50)
        print("üéØ HUMAN DETECTION SYSTEM HELP")
        print("="*50)
        print("üìπ Controls:")
        print("   'q' - Quit the application")
        print("   's' - Save current face to database")
        print("   'h' - Show this help message")
        print("\nüîç Features:")
        print("   ‚Ä¢ Human detection using webcam")
        print("   ‚Ä¢ Gender classification (Male/Female)")
        print("   ‚Ä¢ Face recognition with known persons")
        print("   ‚Ä¢ Age estimation")
        print("   ‚Ä¢ Emotion detection")
        print("   ‚Ä¢ Real-time display with bounding boxes")
        print("\nüé® Display Colors:")
        print("   üü¢ Green box - Recognized person")
        print("   üî¥ Red box - Unknown person")
        print("\nüìä Information Displayed:")
        print("   ‚Ä¢ Recognition status")
        print("   ‚Ä¢ Name (for known persons)")
        print("   ‚Ä¢ Age (hardcoded for known, estimated for unknown)")
        print("   ‚Ä¢ Gender")
        print("   ‚Ä¢ Emotion with emoji")
        print("   ‚Ä¢ Confidence level")
        print("="*50)
    
    def save_encodings(self):
        """Save face encodings to file"""
        encodings = {}
        for name, data in self.known_faces.items():
            if data["encoding"] is not None:
                encodings[name] = data["encoding"]
        
        with open('known_faces/encodings.pkl', 'wb') as f:
            pickle.dump(encodings, f)
    
    def cleanup(self):
        """Clean up resources"""
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()

def main():
    """Main function to run the human detection system"""
    print("Initializing Human Detection and Identification System...")
    print("This system will:")
    print("- Detect if a human is present")
    print("- Identify gender (Male/Female)")
    print("- Recognize known persons with their names and ages")
    print("- Show emotion analysis for unknown persons")
    print("- Display 'I don't know this person' for unrecognized faces")
    
    system = SimpleHumanDetectionSystem()
    
    try:
        system.start_detection()
    except KeyboardInterrupt:
        print("\nSystem stopped by user")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        system.cleanup()

if __name__ == "__main__":
    main() 